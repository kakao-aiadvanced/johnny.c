{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-30T08:00:05.852350Z",
     "start_time": "2024-07-30T07:59:55.196721Z"
    }
   },
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:08:18.024532Z",
     "start_time": "2024-07-30T08:08:17.699735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load document \n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "loader = WebBaseLoader(urls)\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # 청크 크기를 매우 작게 설정합니다. 예시를 위한 설정입니다.\n",
    "    chunk_size=500,\n",
    "    # 청크 간의 중복되는 문자 수를 설정합니다.\n",
    "    chunk_overlap=0,\n",
    "    # 문자열 길이를 계산하는 함수를 지정합니다.\n",
    "    length_function=len,\n",
    "    # 구분자로 정규식을 사용할지 여부를 설정합니다.\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.split_documents(docs)"
   ],
   "id": "58678022b4eeabeb",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:08:30.241233Z",
     "start_time": "2024-07-30T08:08:26.710340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# vector db\n",
    "from langchain_chroma import Chroma\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "db = Chroma.from_documents(documents=texts, embedding=embeddings)\n",
    "retriever = db.as_retriever()\n",
    "query = \"agent memory\"\n",
    "retrieved_docs = retriever.invoke(query)"
   ],
   "id": "4c15c755e3da02e6",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:08:30.248731Z",
     "start_time": "2024-07-30T08:08:30.243405Z"
    }
   },
   "cell_type": "code",
   "source": "query",
   "id": "adfcfe1d79392ce4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agent memory'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:08:38.338786Z",
     "start_time": "2024-07-30T08:08:37.269438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Determine whether query and doc has relevance. If query and doc has relevance, you say yes else no. \\n{format_instructions}\\n{query}\\n{doc}\",\n",
    "    input_variables=[\"query\", \"doc\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"query\": query, \"doc\": retrieved_docs[0].page_content})"
   ],
   "id": "ffe21a3697d7f045",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relevance': 'yes'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:11:04.142877Z",
     "start_time": "2024-07-30T08:11:02.186210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(chain.invoke({\"query\": query, \"doc\": retrieved_docs[1].page_content}))\n",
    "print(chain.invoke({\"query\": query, \"doc\": retrieved_docs[2].page_content}))\n",
    "print(chain.invoke({\"query\": query, \"doc\": retrieved_docs[3].page_content}))"
   ],
   "id": "af51a1d348953f7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevance': 'yes'}\n",
      "{'relevance': 'yes'}\n",
      "{'relevance': 'no'}\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:11:10.956736Z",
     "start_time": "2024-07-30T08:11:10.953241Z"
    }
   },
   "cell_type": "code",
   "source": "retrieved_docs[3].page_content",
   "id": "d9c42e3c1163f8b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:11:57.542393Z",
     "start_time": "2024-07-30T08:11:56.891784Z"
    }
   },
   "cell_type": "code",
   "source": "chain.invoke({\"query\": query, \"doc\": 'blah blah'})",
   "id": "877c861ce2cdbb04",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relevance': 'no'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:18:19.555024Z",
     "start_time": "2024-07-30T08:18:17.481832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ‘yes’ 이고 7의 평가에서도 문제가 없다면, 4의 retrieved chunk 를 가지고 답변 작성 prompt | llm | parser 코드 작성\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "parser = JsonOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Determine whether query and doc has relevance. If query and doc has relevance, you say yes else no\\n{format_instructions}\\n{query}\\n{doc}\",\n",
    "    input_variables=[\"query\", \"doc\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"doc\": retriever | format_docs, \"query\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk)\n",
    "    #print(chunk, end=\"\", flush=True)"
   ],
   "id": "dd76978a645c5237",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'relevance': ''}\n",
      "{'relevance': 'yes'}\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:18:49.617151Z",
     "start_time": "2024-07-30T08:18:49.614116Z"
    }
   },
   "cell_type": "code",
   "source": "rag_chain.stream('test')",
   "id": "b5c3e5e5f78bfeb5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object RunnableSequence.stream at 0x13fcdaac0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:16:46.621277Z",
     "start_time": "2024-07-30T08:16:46.617823Z"
    }
   },
   "cell_type": "code",
   "source": "type(retrieved_docs[0])",
   "id": "71792a701404e9c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "927be3b6c0ee944d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
